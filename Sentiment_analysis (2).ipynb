{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39835290",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from hazm import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "from cleantext import clean\n",
    "import emoji\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "comments = pd.read_csv('/home/ak/hazm-sample/snappfood.csv', sep='\\t', on_bad_lines='skip', encoding='utf-8')\n",
    "comments = comments.drop('Unnamed: 0', axis=1)\n",
    "comments.dropna(inplace=True)\n",
    "comments['label_id'] = comments['label_id'].astype(int)\n",
    "comments.head(10)\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "normalizer = Normalizer()\n",
    "stemmer = Stemmer()\n",
    "tagger = POSTagger(model='./resources/postagger.model')\n",
    "\n",
    "\n",
    "def upper_repl(match):\n",
    "    \"\"\" Convert mask-special tokens to real special tokens \"\"\"\n",
    "    return \" [\" + match.group(1).upper().replace('-', '_') + \"] \"\n",
    "\n",
    "\n",
    "def convert_emoji_to_text(text, delimiters=('[', ']')):\n",
    "    \"\"\" Convert emojis to something readable by the vocab and model \"\"\"\n",
    "    text = emoji.demojize(text, delimiters=delimiters)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_html(raw_html):\n",
    "    \"\"\" Remove all html tags \"\"\"\n",
    "    cleaner = re.compile('<.*?>')\n",
    "    cleaned = re.sub(cleaner, '', raw_html)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def clean_text(\n",
    "        raw_text,\n",
    "        fix_unicode=True,\n",
    "        to_ascii=False,\n",
    "        lower=True,\n",
    "        no_line_breaks=True,\n",
    "        no_urls=True,\n",
    "        no_emails=True,\n",
    "        no_phone_numbers=True,\n",
    "        no_numbers=False,\n",
    "        no_digits=False,\n",
    "        no_currency_symbols=True,\n",
    "        no_punct=False,\n",
    "        replace_with_url=\"\",\n",
    "        replace_with_email=\"\",\n",
    "        replace_with_phone_number=\"\",\n",
    "        replace_with_number=\"\",\n",
    "        replace_with_digit=\"0\",\n",
    "        replace_with_currency_symbol=\"\"):\n",
    "    \"\"\" Preprocessing and normalization the text a the low level \"\"\"\n",
    "    cleaned = clean(\n",
    "        raw_text,\n",
    "        fix_unicode=fix_unicode,\n",
    "        to_ascii=to_ascii,\n",
    "        lower=lower,\n",
    "        no_line_breaks=no_line_breaks,\n",
    "        no_urls=no_urls,\n",
    "        no_emails=no_emails,\n",
    "        no_phone_numbers=no_phone_numbers,\n",
    "        no_numbers=no_numbers,\n",
    "        no_digits=no_digits,\n",
    "        no_currency_symbols=no_currency_symbols,\n",
    "        no_punct=no_punct,\n",
    "        replace_with_url=replace_with_url,\n",
    "        replace_with_email=replace_with_email,\n",
    "        replace_with_phone_number=replace_with_phone_number,\n",
    "        replace_with_number=replace_with_number,\n",
    "        replace_with_digit=replace_with_digit,\n",
    "        replace_with_currency_symbol=replace_with_currency_symbol\n",
    "    )\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def cleaning(\n",
    "        text,\n",
    "        wikipedia=True,\n",
    "        default_cleaning=True,\n",
    "        normalize_cleaning=True,\n",
    "        half_space_cleaning=True,\n",
    "        html_cleaning=True,\n",
    "        emoji_convert=False,\n",
    "        username_cleaning=True,\n",
    "        hashtag_cleaning=True,\n",
    "        fix_unicode=True,\n",
    "        to_ascii=False,\n",
    "        lower=True,\n",
    "        no_line_breaks=True,\n",
    "        no_urls=True,\n",
    "        no_emails=True,\n",
    "        no_phone_numbers=True,\n",
    "        no_numbers=False,\n",
    "        no_digits=False,\n",
    "        no_currency_symbols=True,\n",
    "        no_punct=False,\n",
    "        replace_with_url=\"\",\n",
    "        replace_with_email=\"\",\n",
    "        replace_with_phone_number=\"\",\n",
    "        replace_with_number=\"\",\n",
    "        replace_with_digit=\"0\",\n",
    "        replace_with_currency_symbol=\"\"):\n",
    "    \"\"\" A hierarchy of normalization and preprocessing \"\"\"\n",
    "    text = text.strip()\n",
    "    \n",
    "    if wikipedia:\n",
    "        # If your data extracted from WikiPedia\n",
    "        text = text.replace('_', ' ')\n",
    "        text = text.replace('«', '').replace('»', '')\n",
    "        text = text.replace('[[', '[').replace(']]', ']')\n",
    "        text = text.replace('[ [ ', '[').replace(' ] ]', ']')\n",
    "        text = text.replace(' [ [', ' [').replace('] ] ', '] ')\n",
    "        text = text.replace(' [ [ ', ' [').replace(' ] ] ', '] ')\n",
    "        text = text.replace(' . com', '.com').replace('. com', '.com')\n",
    "        text = text.replace(' . net', '.net').replace('. net', '.net')\n",
    "        text = text.replace(' . org', '.org').replace('. org', '.org')\n",
    "        text = text.replace(' . io', '.io').replace('. io', '.io')\n",
    "        text = text.replace(' . io', '.io').replace('. io', '.io')\n",
    "        text = text.replace('ه ی', 'ه')\n",
    "        text = text.replace('هٔ', 'ه')\n",
    "        text = text.replace('أ', 'ا')\n",
    "\n",
    "    if username_cleaning:\n",
    "        text = re.sub(r\"\\@[\\w.-_]+\", \" \", text)\n",
    "\n",
    "    if hashtag_cleaning:\n",
    "        text = text.replace('#', ' ')\n",
    "        text = text.replace('_', ' ')\n",
    "\n",
    "    if emoji_convert:\n",
    "        text = emoji.emojize(text)\n",
    "        text = convert_emoji_to_text(text)\n",
    "\n",
    "    # regular cleaning\n",
    "    if default_cleaning:\n",
    "        text = clean_text(\n",
    "            text,\n",
    "            fix_unicode,\n",
    "            to_ascii,\n",
    "            lower,\n",
    "            no_line_breaks,\n",
    "            no_urls,\n",
    "            no_emails,\n",
    "            no_phone_numbers,\n",
    "            no_numbers,\n",
    "            no_digits,\n",
    "            no_currency_symbols,\n",
    "            no_punct,\n",
    "            replace_with_url,\n",
    "            replace_with_email,\n",
    "            replace_with_phone_number,\n",
    "            replace_with_number,\n",
    "            replace_with_digit,\n",
    "            replace_with_currency_symbol\n",
    "        )\n",
    "\n",
    "    # cleaning HTML\n",
    "    if html_cleaning:\n",
    "        text = clean_html(text)\n",
    "\n",
    "    # normalizing\n",
    "    if normalize_cleaning:\n",
    "        text = normalizer.normalize(text)\n",
    "\n",
    "    # removing weird patterns\n",
    "    weird_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u'\\U00010000-\\U0010ffff'\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\u3030\"\n",
    "        u\"\\ufe0f\"\n",
    "        u\"\\u2069\"\n",
    "        u\"\\u2066\"\n",
    "        u\"\\u2013\"\n",
    "        u\"\\u2068\"\n",
    "        u\"\\u2067\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    text = weird_pattern.sub(r'', text)\n",
    "\n",
    "    # removing extra spaces, hashtags\n",
    "    text = re.sub(\"#\", \"\", text)\n",
    "    # text = re.sub(\"\\s+\", \" \", text)\n",
    "\n",
    "    if emoji_convert:\n",
    "        text = re.sub(r\"\\[(\\w.+)\\]\", upper_repl, text)\n",
    "        # text = re.sub(\"\\s+\", \" \", text)\n",
    "\n",
    "    if half_space_cleaning:\n",
    "        text = text.replace('\\u200c', ' ')\n",
    "        # text = re.sub(\"\\s+\", \" \", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def sent_tokenizer(text, cleaning_fn=None, return_status=False):\n",
    "    text = cleaning_fn(text) if callable(cleaning_fn) else text\n",
    "    _words = word_tokenize(text)\n",
    "    words = tagger.tag(_words)\n",
    "    items = list(filter(lambda w: (w[1][1] == 'V') and ((len(words) > (w[0] + 1)) and (words[w[0] + 1][0] in \"!.?⸮؟\")),\n",
    "                        enumerate(words)))\n",
    "    # items = list(filter(lambda w: (w[1][1] == 'V') and ((words[w[0] + 1][0] in \"!.?⸮؟\")), enumerate(words)))\n",
    "    sid = list(sorted(map(lambda w: w[0] + 1, items)))\n",
    "    sentences = []\n",
    "\n",
    "    if not len(sid) > 0:\n",
    "        if return_status:\n",
    "            return False, [text]\n",
    "        return [text]\n",
    "\n",
    "    sid = list(sorted(list(set([0] + sid + [len(_words) - 1]))))\n",
    "    for i in range(0, len(sid) - 1):\n",
    "        if i == 0:\n",
    "            start = 0\n",
    "            end = sid[i + 1]\n",
    "        else:\n",
    "            start = sid[i]\n",
    "            end = sid[i + 1]\n",
    "\n",
    "        ss = _words[start: end]\n",
    "        s = ' '.join(ss[:-1])\n",
    "        s = s.replace('_', ' ').replace('_', ' ')\n",
    "        s = s.replace(' . ', '.')\n",
    "        s = s.replace('( ', ' (').replace(' )', ') ')\n",
    "        s = s + ' ' + ss[-1].replace('_', ' ')\n",
    "        s = re.sub('\\s\\s+', ' ', s)\n",
    "\n",
    "        sentences.append(s)\n",
    "\n",
    "    if return_status:\n",
    "        return True, sentences\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def sent_tokenizer_v2(text):\n",
    "    words = tagger.tag(word_tokenize(text))\n",
    "\n",
    "    items = list(filter(lambda w: (w[1][1] == 'V') and ((len(words) > (w[0] + 1)) and (words[w[0] + 1][0] in \"!.?⸮؟\")),\n",
    "                        enumerate(words)))\n",
    "    ids = list(map(lambda w: w[0] + 1, items))\n",
    "    sentences = []\n",
    "\n",
    "    for i in range(len(ids)):\n",
    "        if i == 0:\n",
    "            if ids[i] > 0:\n",
    "                start = 0\n",
    "                end = ids[i] + 1\n",
    "        else:\n",
    "            start = ids[i - 1] + 1\n",
    "            end = ids[i] + 1\n",
    "\n",
    "        ss = list(map(lambda w: w[0], words[start: end]))\n",
    "        s = ' '.join(ss[:-1])\n",
    "        s = s.replace('_', ' ').replace('_', ' ')\n",
    "        s = s.replace(' . ', '.')\n",
    "        s = s.replace('( ', ' (').replace(' )', ') ')\n",
    "        # s = s + ' ' + ss[-1].replace('_', ' ')\n",
    "        s = re.sub('\\s\\s+', ' ', s)\n",
    "        sentences.append(s)\n",
    "\n",
    "    return sentences\n",
    "    \n",
    "    \n",
    "    \n",
    "def preprocessing_text(string):\n",
    "    # result = upper_repl(string)\n",
    "    result = convert_emoji_to_text(string)\n",
    "    result = clean_html(result)\n",
    "    result = clean_text(result)\n",
    "    result = cleaning(result)\n",
    "    # result = sent_tokenizer(result)\n",
    "    # result = sent_tokenizer_v2(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "comments['comment'] = comments['comment'].apply(lambda cw: preprocessing_text(cw))\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "feedbacks = comments['comment'].values\n",
    "sentiment = comments['label'].values\n",
    "encoder = LabelEncoder()\n",
    "encoded_labels = encoder.fit_transform(sentiment)\n",
    "\n",
    "train_sentences, test_sentences, train_labels, test_labels = train_test_split(feedbacks, encoded_labels, stratify=encoded_labels)\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "vec = CountVectorizer(max_features = 3000)\n",
    "X = vec.fit_transform(train_sentences)\n",
    "vocab = vec.get_feature_names_out()\n",
    "X = X.toarray()\n",
    "\n",
    "word_counts = {}\n",
    "for l in range(2):\n",
    "    word_counts[l] = defaultdict(lambda: 0)\n",
    "for i in range(X.shape[0]):\n",
    "    l = train_labels[i]\n",
    "    for j in range(len(vocab)):\n",
    "        word_counts[l][vocab[j]] += X[i][j]\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "def laplace_smoothing(n_label_items, vocab, word_counts, word, text_label):\n",
    "    a = word_counts[text_label][word] + 1\n",
    "    b = n_label_items[text_label] + len(vocab)\n",
    "    return math.log(a/b)\n",
    "\n",
    "def group_by_label(x, y, labels):\n",
    "    data = {}\n",
    "    for l in labels:\n",
    "        data[l] = x[np.where(y == l)]\n",
    "    return data\n",
    "\n",
    "\n",
    "def fit(x, y, labels):\n",
    "    n_label_items = {}\n",
    "    log_label_priors = {}\n",
    "    n = len(x)\n",
    "    grouped_data = group_by_label(x, y, labels)\n",
    "    for l, data in grouped_data.items():\n",
    "        n_label_items[l] = len(data)\n",
    "        log_label_priors[l] = math.log(n_label_items[l] / n)\n",
    "    return n_label_items, log_label_priors\n",
    "\n",
    "\n",
    "def predict(n_label_items, vocab, word_counts, log_label_priors, labels, x):\n",
    "    result = []\n",
    "    for text in x:\n",
    "        label_scores = {l: log_label_priors[l] for l in labels}\n",
    "        words = set(word_tokenize(text))\n",
    "        for word in words:\n",
    "            if word not in vocab: continue\n",
    "            for l in labels:\n",
    "                log_w_given_l = laplace_smoothing(n_label_items, vocab, word_counts, word, l)\n",
    "                label_scores[l] += log_w_given_l\n",
    "        result.append(max(label_scores, key=label_scores.get))\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "labels = [0,1]\n",
    "\n",
    "n_label_items, log_label_priors = fit(train_sentences,train_labels,labels)\n",
    "pred = predict(n_label_items, vocab, word_counts, log_label_priors, labels, test_sentences)\n",
    "print(\"Accuracy of prediction on test set : \", accuracy_score(test_labels,pred))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
